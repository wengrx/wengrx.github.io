<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">


<meta name="keywords" content="Rongxiang Weng, Weng Rongxiang, wengrx"> 
<meta name="description" content="Rongxiang Weng&#39;s home page">
<meta name="google-site-verification" content="yy_3iiS_X6pJdegdwitJMrH0LRLHXwpjrV9RKLXxKjg">
<meta name="google-site-verification" content="yy_3iiS_X6pJdegdwitJMrH0LRLHXwpjrV9RKLXxKjg">
<link rel="stylesheet" href="./wengrx_contents/jemdoc.css" type="text/css">
<title>Rongxiang Weng, Alibaba Inc</title>
<script async="" src="./wengrx_contents/analytics.js"></script><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-87320911-1', 'auto');
  ga('send', 'pageview');

</script>
<script type="text/javascript" src="./wengrx_contents/jquery-1.12.4.min.js"></script></head>
<body>
<div id="layout-content" style="margin-top:25px">

<table>
	<tbody>
		<tr>
			<td width="670">
				<div id="toptitle">					
					<h1>Rongxiang Weng (翁荣祥)</h1><h1>
				</h1></div>

				<!-- <h3>NLP Researcher</h3> -->
				<p>
					Machine Intelligence Technology Lab<br>
					Alibaba DAMO Academy <br>
					Hangzhou, China<br>
					<br>
					Email: wengrongxiang at gmail dot com <br>
				</p>
			</td>
			<td>
				<img src="./wengrx_contents/my.jpg" border="0" width="160"><br>
			</td>
		</tr><tr>
	</tr></tbody>
</table>

<h2>Biography</h2>
<p>
	I am a NLP researcher at Machine Intelligence Technology Lab, <a href="https://damo.alibaba.com/">Alibaba DAMO Acadamy</a>.

</p>
<p>My research interests include Machine Translation, Natural Language Generation and Deep Learning. Currently, I focus on the controllable language generation and cross-lingual representation learning.
</p>
	<p>
We are recruiting research interns and full-time employees in machine translation, text generation, cross-lingual pre-training and other multi-lingual tasks. If you are interested in, please feel free to contact me.
</p>


<h2>News</h2>
<ul>    <li> [01/2021] Our paper "On Learning Universal Representations Across Languages" has been accepted by ICLR 2021. </li>
		<li>[09/2020] Two paper "Towards Enhancing Faithfulness for Neural Machine Translation" and "Uncertainty-Aware Semantic Augmentation for Neural Machine Translation" have been accepted by EMNLP 2020. </li>
	    <li>[06/2020] Our paper "Improving Self-Attention Networks with Sequential Relations" has been accepted by IEEE TASLP. </li>
		<li>[04/2020]  Our paper "Multiscale Collaborative Deep Models for Neural Machine Translation" has been accepted by ACL 2020.
			</li>
		<li>[11/2019]  Two paper "GRET: Global Representation Enhanced Transformer" and "Acquiring Knowledge from Pre-trained Model to Neural Machine Translation" have been accepted by AAAI 2020.
			</li>
		<li>
			  [05/2019] Our paper "Learning Representation Mapping for Relation Detection in Knowledge Base Question Answering" has been accepted by ACL 2019.
			</li>
		<li>
			  [05/2019] Our paper "Correct-and-Memorize: Learning to Translate from Interactive Revisions" has been accepted by IJCAI 2019.
			</li>
		<li>
			  [10/2018] Our paper "Discriminating Noises for Better Incorporating External Information in Neural Machine Translation" has been released in Arxiv.	
			</li>
		<li>
				[07/2017] Our paper "Neural Machine Translation with Word Predictions" has been accepted by EMNLP 2017. 
			</li>
</ul>



<h2>Publication </h2>
<ul>
	<li>Xiangpeng Wei, <b>Rongxiang Weng</b>, Yue Hu, Luxi Xing, Heng Yu and Weihua Luo. 
		"On Learning Universal Representations Across Languages". In ICLR 2021. <p>[<a href="https://ieeexplore.ieee.org/abstract/document/9099090/" target="_blank">paper</a>]
	</li>
	<li><b>Rongxiang Weng</b>, Heng Yu, Xiangpeng Wei and Weihua Luo.
		"Towards Enhancing Faithfulness for Neural Machine Translation". In EMNLP 2020. <p>[<a href="https://ieeexplore.ieee.org/abstract/document/9099090/" target="_blank">paper</a>]
	</li>
	<li>Xiangpeng Wei, Yue Hu, <b>Rongxiang Weng</b>, Heng Yu and Weihua Luo.
		"Uncertainty-Aware Semantic Augmentation for Neural Machine Translation". In EMNLP 2020. <p>[<a href="https://ieeexplore.ieee.org/abstract/document/9099090/" target="_blank">paper</a>]
	</li>
	<li>Zaixiang Zheng, Shujian Huang, <b>Rongxiang Weng</b>, Xinyu Dai and Jiajun Chen. 
		"Improving Self-Attention Networks with Sequential Relations". In IEEE TASLP. <p>[<a href="https://ieeexplore.ieee.org/abstract/document/9099090/" target="_blank">paper</a>]
	</li>
	<li>
		Xiangpeng Wei, Heng Yu, Yue Hu, Yue Zhang, <b>Rongxiang Weng</b> and Weihua Luo.
		"Multiscale Collaborative Deep Models for Neural Machine Translation". In ACL 2020. <p>[<a href="https://arxiv.org/pdf/2004.14021.pdf" target="_blank">paper</a>][<a href="https://github.com/pemywei/MSC-NMT" target="_blank">code</a>]
	</li>
	<li>
			<b>Rongxiang Weng</b>, Heng Yu, Shujian Huang, Shanbo Cheng and Weihua Luo. "Acquiring Knowledge from Pre-trained Model to Neural Machine Translation". In AAAI 2020.<p>[<a href="https://arxiv.org/pdf/1912.01774.pdf" target="_blank">paper</a>]
	</li>
	<li>
			<b>Rongxiang Weng</b>, Haoran Wei, Shujian Huang, Heng Yu, Lidong Bing, Weihua Luo and Jiajun Chen. "GRET: Global Representation Enhanced Transformer". In AAAI 2020.<p>[<a href="https://arxiv.org/pdf/2002.10101.pdf" target="_blank">paper</a>]
	</li>
	<li>
			Peng Wu, Shujian Huang, <b>Rongxiang Weng</b>, Zaixiang Zheng, Jianbing Zhang, Xiaohui Yan and Jiajun Chen. "Learning Representation Mapping for Relation Detection in Knowledge Base Question Answering". In ACL 2019.
			<p>[<a href="https://arxiv.org/pdf/1907.07328.pdf" target="_blank">paper</a>]
				[<a href="https://github.com/wudapeng268/KBQA-Adapter" target="_blank">code</a>]
	</li>
	<li>
			<b>Rongxiang Weng</b>, Hao Zhou, Shujian Huang, Lei Li, Yifan Xia and Jiajun Chen. "Correct-and-Memorize: Learning to Translate from Interactive Revisions". In IJCAI 2019.
			<p>[<a href="https://arxiv.org/pdf/1907.03468.pdf" target="_blank">paper</a>]
			[<a href="https://github.com/wengrx/CAMIT" target="_blank">code</a>]
	</li>
	<li>
			<b>Rongxiang Weng</b>, Shujiang Huang, Zaixiang Zheng, Xinyu Dai and Jiajun Chen. "Neural Machine Translation with Word Predictions". In EMNLP 2017. 
			<p>[<a href="https://arxiv.org/abs/1708.01771" target="_blank">paper</a>]
	</li>
</ul>
<h2>Pre-print </h2>
<ul>
<li>
	Shanbo Cheng, Shaohui Kuang, <b>Rongxiang Weng</b>, Heng Yu, Changfeng Zhu and Weihua Luo.
	"AR: Auto-Repair the Synthetic Data for Neural Machine Translation" arXiv Preprint, 2020.
</li>
<li>
	<b>Rongxiang Weng</b>, Heng Yu, Shujian Huang, Weihua Luo and Jiajun Chen.
	"Improving Neural Machine Translation with Pre-trained Representation" arXiv Preprint, 2019.
</li>
<li>
	Zaixiang Zheng, Shujiang Huang, Zewei Sun, <b>Rongxiang Weng</b>, Xinyu Dai and Jiajun Chen. "Discriminating Noises for Better Incorporating External Information in Neural Machine Translation" arXiv Preprint, 2018.
</li>
</ul>
<div id="footer">
	<div id="footer-text"></div>
</div>
	<p></p><center>
		<a href="https://clustrmaps.com/site/1afgn" title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=t4Be0FJJSJcdltyk6gBwu6DSM2JANWzn_mOrZSLHoqk&cl=ffffff"></a>
	<br>
        © Rongxiang Weng | Last updated: 31/07/2019
     
      </center><p></p>


</div>

<div class="jvectormap-tip"></div></body></html>